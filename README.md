Minesweeper is a computer puzzle game popularized in 1990 by Microsoft that is typically solved with deduction and spatial reasoning. Though logical solvers solve minesweeper to a high degree of accuracy, computational power and speed often limit scaling to extremely large, dense boards. As a result, we propose that training a solver through reinforcement learning and pre-training can mitigate these limitations. In this paper, we provide a proof-of-concept idea for how expert data gathered from existing logic solvers can be used to train a more effective model using Proximal Policy Optimization. Additionally, we have initial evidence that neural policies may be faster at decision-making when solving minesweeper boards compared to logical solvers.

This project attempts to perform behavioral cloning on expert data from the logical solver, and performs the DAgger algorithm followed by PPO, and compares it to a simple PPO algorithm. 
